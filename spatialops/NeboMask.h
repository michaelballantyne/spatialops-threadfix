/* This file was generated by fulmar version 0.9.0. */

/*
 * Copyright (c) 2014 The University of Utah
 *
 * Permission is hereby granted, free of charge, to any person obtaining a copy
 * of this software and associated documentation files (the "Software"), to
 * deal in the Software without restriction, including without limitation the
 * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
 * sell copies of the Software, and to permit persons to whom the Software is
 * furnished to do so, subject to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included in
 * all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
 * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
 * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
 * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
 * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
 * IN THE SOFTWARE.
 */

#ifndef NEBO_MASK_H
#  define NEBO_MASK_H

   namespace SpatialOps {
      template<typename CurrentMode, typename FieldType>
       struct NeboMask;
      template<typename FieldType>
       struct NeboMask<Initial, FieldType> {
         public:
          FieldType typedef field_type;

          NeboMask<SeqWalk, FieldType> typedef SeqWalkType;

#         ifdef FIELD_EXPRESSION_THREADS
             NeboMask<Resize, FieldType> typedef ResizeType;
#         endif
          /* FIELD_EXPRESSION_THREADS */

#         ifdef __CUDACC__
             NeboMask<GPUWalk, FieldType> typedef GPUWalkType;
#         endif
          /* __CUDACC__ */

          NeboMask<Reduction, FieldType> typedef ReductionType;

          NeboMask(structured::SpatialMask<FieldType> const & m)
          : mask_(m)
          {}

          inline structured::GhostData possible_ghosts(void) const {
             return mask_.get_valid_ghost_data() + point_to_ghost(mask_.boundary_info().has_extra());
          }

          inline SeqWalkType init(structured::IntVec const & minus,
                                  structured::IntVec const & plus,
                                  structured::IntVec const & shift) const {
             return SeqWalkType(resize_ghost_and_shift_window(mask_,
                                                              minus,
                                                              plus - mask_.boundary_info().has_extra(),
                                                              shift));
          }

#         ifdef FIELD_EXPRESSION_THREADS
             inline ResizeType resize(structured::IntVec const & minus,
                                      structured::IntVec const & plus) const {
                return ResizeType(resize_ghost(mask_, minus, plus - mask_.boundary_info().has_extra()));
             }
#         endif
          /* FIELD_EXPRESSION_THREADS */

#         ifdef __CUDACC__
             inline bool cpu_ready(void) const {
                return mask_.find_consumer(LOCAL_RAM, 0);
             }

             inline bool gpu_ready(int const deviceIndex) const {
                return mask_.find_consumer(EXTERNAL_CUDA_GPU, deviceIndex);
             }

             inline GPUWalkType gpu_init(structured::IntVec const & minus,
                                         structured::IntVec const & plus,
                                         structured::IntVec const & shift,
                                         int const deviceIndex) const {
                return GPUWalkType(deviceIndex,
                                   resize_ghost_and_shift_window(mask_,
                                                                 minus,
                                                                 plus - mask_.boundary_info().has_extra(),
                                                                 shift));
             }

#            ifdef NEBO_GPU_TEST
                inline void gpu_prep(int const deviceIndex) const {
                    std::cout << "1. NeboMask add_consumer() \n";
                   const_cast<structured::SpatialMask<FieldType> *>(&mask_)->
                   add_consumer(EXTERNAL_CUDA_GPU, deviceIndex);
                    std::cout << "2. NeboMask add_consumer() \n";
                }
#            endif
             /* NEBO_GPU_TEST */
#         endif
          /* __CUDACC__ */

          inline ReductionType reduce_init(structured::IntVec const & minus,
                                           structured::IntVec const & plus,
                                           structured::IntVec const & shift) const {
             return ReductionType(resize_ghost_and_shift_window(mask_,
                                                                minus,
                                                                plus - mask_.boundary_info().has_extra(),
                                                                shift));
          }

         private:
          structured::SpatialMask<FieldType> const mask_;
      };
#     ifdef FIELD_EXPRESSION_THREADS
         template<typename FieldType>
          struct NeboMask<Resize, FieldType> {
            public:
             FieldType typedef field_type;

             NeboMask<SeqWalk, FieldType> typedef SeqWalkType;

             NeboMask(structured::SpatialMask<FieldType> const & m)
             : mask_(m)
             {}

             inline SeqWalkType init(structured::IntVec const & shift,
                                     structured::IntVec const & split,
                                     structured::IntVec const & location) const {
                return SeqWalkType(shift_window(structured::SpatialMask<FieldType>(mask_.window_with_ghost().refine(split,
                                                                                                                    location),
                                                                                   mask_),
                                                shift));
             }

            private:
             structured::SpatialMask<FieldType> const mask_;
         }
#     endif
      /* FIELD_EXPRESSION_THREADS */;
      template<typename FieldType>
       struct NeboMask<SeqWalk, FieldType> {
         public:
          FieldType typedef field_type;

          typename field_type::value_type typedef value_type;

          NeboMask(structured::SpatialMask<FieldType> const & m)
          : iter_(m.begin())
          {}

          inline void next(void) { iter_++; }

          inline value_type eval(void) const { return *iter_; }

         private:
          typename structured::SpatialMask<FieldType>::const_iterator iter_;
      };
#     ifdef __CUDACC__
         template<typename FieldType>
          struct NeboMask<GPUWalk, FieldType> {
            public:
             FieldType typedef field_type;

             typename field_type::value_type typedef value_type;

             NeboMask(int const deviceIndex,
                      structured::SpatialMask<FieldType> const & m)
             : bitField_(m.mask_values(EXTERNAL_CUDA_GPU, deviceIndex)),
               offset_(m.window_with_ghost().offset(0) + m.window_with_ghost().glob_dim(0)
               * (m.window_with_ghost().offset(1) + (m.window_with_ghost().glob_dim(1)
                                                     * m.window_with_ghost().offset(2)))),
               bitPosition_(0),
               blockPosition_(0),
               xLength_(m.window_with_ghost().glob_dim(0)),
               step_(xLength_ * m.window_with_ghost().glob_dim(1))
             {}

             __device__ inline void start(int x, int y) {
                bitPosition_ = offset_ + x + y * xLength_;

                update_positions();
             }

             __device__ inline void next(void) {
                bitPosition_ += step_;

                update_positions();
             }

             __device__ inline bool eval(void) const {
                return !(!(*(bitField_ + blockPosition_) & (1 << bitPosition_)));
             }

            private:
             __device__ inline void update_positions(void) {
                if(bitPosition_ < 0 || bitPosition_ >= NEBO_INT_BIT) {
                   const int flatPosition = blockPosition_ * NEBO_INT_BIT +
                   bitPosition_;

                   blockPosition_ = flatPosition / NEBO_INT_BIT;

                   bitPosition_ = flatPosition % NEBO_INT_BIT;
                };
             }

             unsigned int const * bitField_;

             int const offset_;

             int bitPosition_;

             int blockPosition_;

             int const xLength_;

             int const step_;
         }
#     endif
      /* __CUDACC__ */;
      template<typename FieldType>
       struct NeboMask<Reduction, FieldType> {
         public:
          FieldType typedef field_type;

          typename field_type::value_type typedef value_type;

          NeboMask(structured::SpatialMask<FieldType> const & m)
          : iter_(m.begin()), end_(m.end())
          {}

          inline void next(void) { iter_++; }

          inline bool at_end(void) const { return iter_ == end_; }

          inline bool has_length(void) const { return true; }

          inline value_type eval(void) const { return *iter_; }

         private:
          typename structured::SpatialMask<FieldType>::const_iterator iter_;

          typename structured::SpatialMask<FieldType>::const_iterator const end_
          ;
      };
   } /* SpatialOps */

#endif
/* NEBO_MASK_H */
